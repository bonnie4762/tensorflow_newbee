{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # make csv file with stupid order\n",
    "Train='/opt/jupyterhub/SkinData/TrainingSet/'\n",
    "Valid='/opt/jupyterhub/SkinData/ValidationSet/'\n",
    "Test='/opt/jupyterhub/SkinData/TestSet/'\n",
    "Train_csv='./fengkai/train_csv'\n",
    "Valid_csv='./fengkai/valid_csv'\n",
    "Test_csv='./fengkai/test_csv'\n",
    "def batch_img_label(base_path,output_file):\n",
    "    if 'Train' in base_path:\n",
    "        img_1,img_2,img_3 = os.listdir(base_path)\n",
    "    else:\n",
    "        img_1,img_2,img_3,_ = os.listdir(base_path)\n",
    "    print(img_1,img_2,img_3)\n",
    "    with open (output_file, 'a') as csvfile:\n",
    "        headers = ['image_id', 'Seborrheic', 'Nevus','Melanoma']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers) \n",
    "        writer.writeheader()\n",
    "\n",
    "\n",
    "        if not os.path.isfile(output_file):\n",
    "            writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "\n",
    "        for file_name in os.listdir(os.path.join(base_path,img_1)):\n",
    "            writer.writerow({'image_id':os.path.join(os.path.join(base_path,img_1),file_name),'Seborrheic':1,'Nevus':0,'Melanoma':0})   \n",
    "        for file_name in os.listdir(os.path.join(base_path,img_2)):\n",
    "            writer.writerow({'image_id':os.path.join(os.path.join(base_path,img_2),file_name),'Seborrheic':0,'Nevus':1,'Melanoma':0})  \n",
    "        for file_name in os.listdir(os.path.join(base_path,img_3)):\n",
    "            writer.writerow({'image_id':os.path.join(os.path.join(base_path,img_3),file_name),'Seborrheic':0,'Nevus':0,'Melanoma':1})  \n",
    "\n",
    "# batch_img_label(Train,Train_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alexnet \n",
    "\n",
    "# check bn before relu...\n",
    "def alexnet_full_connect(inputs,\n",
    "               num_classes=3,\n",
    "               is_training=True,\n",
    "               dropout_keep_prob=0.5,\n",
    "               spatial_squeeze=True,\n",
    "               scope='alexnet_connect',\n",
    "               global_pool=False):\n",
    "\n",
    "    conv1=tf.layers.conv2d(inputs, 64, [11, 11], 4, padding='VALID')\n",
    "    conv1=tf.layers.batch_normalization(conv1)\n",
    "    pool1=tf.layers.max_pooling2d(conv1, [3, 3], 2)\n",
    "    \n",
    "    conv2=tf.layers.conv2d(pool1,192, [5, 5])\n",
    "    conv2=tf.layers.batch_normalization(conv2)\n",
    "    pool2=tf.layers.max_pooling2d(conv2,[3, 3], 2)\n",
    "    \n",
    "    conv3=tf.layers.conv2d(pool2,384, [3, 3])\n",
    "    conv3=tf.layers.batch_normalization(conv3)\n",
    "    conv4=tf.layers.conv2d(conv3, 384, [3, 3])\n",
    "    conv4=tf.layers.batch_normalization(conv4)\n",
    "    conv5=tf.layers.conv2d(conv4,256, [3, 3])\n",
    "    conv5=tf.layers.batch_normalization(conv5)\n",
    "    pool5=tf.layers.max_pooling2d(conv5,[3, 3], 2)\n",
    "    \n",
    "    flatten1=tf.contrib.layers.flatten(pool5)\n",
    "    full_connect6=tf.contrib.layers.fully_connected(flatten1,num_outputs=4096)\n",
    "    full_connect6=tf.layers.batch_normalization(full_connect6)\n",
    "    drop_out1=tf.layers.dropout(full_connect6,dropout_keep_prob)\n",
    "    full_connect7=tf.contrib.layers.fully_connected(drop_out1,num_outputs=4096)\n",
    "    full_connect7=tf.layers.batch_normalization(full_connect7)\n",
    "    drop_out2=tf.layers.dropout(full_connect7,dropout_keep_prob)\n",
    "    full_connect8=tf.contrib.layers.fully_connected(drop_out2,num_outputs=num_classes)\n",
    "    full_connect8=tf.layers.batch_normalization(full_connect8)\n",
    "#     output=tf.nn.sigmoid(full_connect8)\n",
    "    return full_connect8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_csv_batch(csv_file,batch_size,if_aug):\n",
    "    \n",
    "    shitname = tf.train.string_input_producer([csv_file],shuffle=True)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(shitname)\n",
    "\n",
    "    value_shuffle=tf.random_shuffle(value)\n",
    "    file_name,Seborrheic,Nevus,Melanoma = tf.decode_csv(value_shuffle,record_defaults=[[\"\"],[0],[0],[0]])\n",
    "    #one_hot_label\n",
    "      #  file_name=tf.as_string(file_name).decode('ascii')\n",
    "    one_hot_label=tf.stack([Seborrheic,Nevus,Melanoma])\n",
    "\n",
    "\n",
    "    image = tf.train.string_input_producer([file_name],shuffle=False)\n",
    "\n",
    "    img_reader = tf.WholeFileReader()\n",
    "    #     print(input_queue[0])\n",
    "    filename, b_img = img_reader.read(image)\n",
    "    real_image=tf.image.decode_jpeg(b_img,channels=3)\n",
    "\n",
    "    img = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(img, 224, 224) \n",
    "    resized_image = tf.image.per_image_standardization(resized_image)\n",
    "    if if_aug:\n",
    "        resized_image = tf.image.random_flip_left_right(resized_image)\n",
    "        \n",
    "    #resized_image= tf.image.flip_left_right(resized_image)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    min_after_dequeue = 60\n",
    "    capacity = min_after_dequeue + 1 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [resized_image,one_hot_label], batch_size=batch_size, capacity=capacity,\n",
    "      min_after_dequeue=min_after_dequeue) \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "    Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        # to stablize the rank\n",
    "        lbls= tf.argmax(labels, 1)\n",
    "        correct = tf.nn.in_top_k(logits, lbls, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # # # # # # # # # # # # # # # start training  # # # # # # # # # # # # # # #\n"
     ]
    }
   ],
   "source": [
    "#enable gpu growth\n",
    "logs_dir='./fengkai/alexnet_log/'\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.mkdir(logs_dir)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "result_log='./fengkai/result.txt'\n",
    "\n",
    "if not os.path.exists(result_log):\n",
    "    open(result_log, 'a').close()\n",
    "with session as sess:\n",
    "    # data training flow\n",
    "\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, 224, 224, 3], name=\"image_shit\")\n",
    "    y = tf.placeholder(tf.int32, shape=[None, 3], name=\"label_shit\")\n",
    "    pred=alexnet_full_connect(X)\n",
    "    batch_size=4\n",
    "\n",
    "\n",
    "    # train op\n",
    "    loss = tf.losses.softmax_cross_entropy(y, logits=pred)\n",
    "    global_step = tf.train.get_or_create_global_step() #if it is default, learning rate should be 0.001\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(0.001,global_step,10,0.98,staircase =True)\n",
    " \n",
    "    optim=tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(loss,global_step)\n",
    "#momentum check \n",
    "\n",
    "###############\n",
    "    validate_op=evaluation(pred, y)\n",
    "    test_op=evaluation(pred,y)\n",
    "    \n",
    "    train_csv_tensor=tf.convert_to_tensor(Train_csv,dtype=tf.string)\n",
    "    img_after_csv,lbl_after_csv=read_csv_batch(train_csv_tensor,batch_size,if_aug=True)\n",
    "    \n",
    "    valid_csv_tensor=tf.convert_to_tensor(Valid_csv,dtype=tf.string)\n",
    "    valid_img,valid_label=read_csv_batch(valid_csv_tensor,batch_size,if_aug=False)\n",
    "    \n",
    "    test_csv_tensor=tf.convert_to_tensor(Test_csv,dtype=tf.string)\n",
    "    test_img,test_label=read_csv_batch(test_csv_tensor,batch_size,if_aug=False)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer=tf.summary.FileWriter(logs_dir, sess.graph)\n",
    "    \n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    loss_epoch=0\n",
    "    \n",
    "    print('{} start training {}'.format(15 *' #',15 *' #'))\n",
    "    with open(result_log,'a') as result_file:\n",
    "        \n",
    "        for iter in range (0, 2000,batch_size):\n",
    "            img_batch,lbl_batch=sess.run([img_after_csv,lbl_after_csv])\n",
    "            #val_img_batch,val_lbl_batch=sess.run([valid_img,valid_label])\n",
    "            _,train_loss=sess.run([optim,loss],feed_dict={X: img_batch,\n",
    "                                   y: lbl_batch\n",
    "                                   })\n",
    "#             print('label values{}'.format(lbl_batch))\n",
    "            #acc=sess.run([validate_op],feed_dict={X: val_img_batch, y: val_lbl_batch})            \n",
    "\n",
    "            if iter % (20)==0:\n",
    "                acc_epoch=0\n",
    "                test_av_accuracy=0\n",
    "                for val_i in range(0,150,batch_size):\n",
    "                    val_img_batch,val_lbl_batch=sess.run([valid_img,valid_label])\n",
    "                    acc=sess.run([validate_op],feed_dict={X: val_img_batch, y: val_lbl_batch})\n",
    "                    acc_epoch+=acc[0]\n",
    "                for t in range(0, 600,batch_size):\n",
    "                    test_img_batch,test_lbl_batch=sess.run([test_img,test_label])\n",
    "                    test_acc=sess.run([test_op],feed_dict={X: test_img_batch,\n",
    "                                               y: test_lbl_batch\n",
    "                                               }) \n",
    "                    test_av_accuracy+=test_acc[0]\n",
    "        \n",
    "                print('iter{},loss{},accuracy on validation{} on test{}'.format(iter,train_loss,acc_epoch/(150/batch_size),test_av_accuracy/(600/batch_size)) )\n",
    "                \n",
    "                result_file.write('iter{},loss{},accuracy on validation{}on test{}\\n'.format(iter,train_loss,acc_epoch/(150/batch_size),test_av_accuracy/(600/batch_size)) )\n",
    "                \n",
    "                #print('the trainning loss and accuracy are {} and {}'.format(loss[0],acc[0]))\n",
    "            if iter % (500) ==0:\n",
    "                checkpoint_path = os.path.join(logs_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=iter)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
